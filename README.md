# âœ¨ Transformer Decoder from Scratch â€“ Tiny Shakespeare

> **â€œI built a GPTâ€‘like decoder architecture from the ground upâ€”no shortcuts, no preâ€‘made models. Just raw PyTorch, math, and curiosity.â€**

This project is a handsâ€‘on deep dive into transformer decoders. I wanted to understand every internalâ€”multiâ€‘head attention, feedâ€‘forward nets, layer norms, training dynamics, and visualization. So I built it from scratch and trained it on the Tiny Shakespeare dataset to generate text.  
This is not just a demo. Itâ€™s a **reproducible, educational, and extensible framework** for decoderâ€‘only models.

---

## ðŸ“Œ Highlights

- âœ… **Fromâ€‘scratch decoder architecture** (Multiâ€‘Head Attention, FFN, LayerNorm, Dropout, Masking)  
- ðŸ“š **Trained on Tiny Shakespeare** with a real tokenizer (`bertâ€‘baseâ€‘uncased`)  
- ðŸŽ¯ Implements **causal attention**, weight initialization, positional embeddings, and a manual training loop  
- ðŸŽ¨ **Attention visualization** tool to inspect how the model distributes focus across tokens  

---

## ðŸ” Whatâ€™s Inside

| Module                                   | Description                                               |
|------------------------------------------|-----------------------------------------------------------|
| Import_libraries.py                      | All necessary imports (PyTorch, Transformers, etc.)       |
| Feed_Forward_Network.py                  | Twoâ€‘layer FFN with Xavier init and ReLU                   |
| MultiHeadAttn.py                         | Perâ€‘head multiâ€‘head attention with masking                |
| Decoder_Block.py                         | Decoder block combining MHA + FFN + LayerNorm + residuals |
| FC_Layer(Final_layer).py                 | Final linear projection to vocabulary size                |
| DecoderOnlyModel(...).py                 | Assembles full decoder: token + positional embeddings     |
| Train.py                                 | Custom training loop (tokenization â†’ dataloader â†’ train)  |
| atten_image.py                           | Attention heatmap visualizer per layer & head             |
| tiny_transformer_weights.pth             | Trained model weights                                     |
| Requirement.txt                          | Python dependencies                                       |

---

## ðŸ—ï¸ Model Architecture

> **I didnâ€™t use** nn.Transformer **or** nn.MultiheadAttentionâ€”everything is built manually.

Each DecoderBlock consists of:

1. **Multiâ€‘Head Attention** (from scratch) â€“ Q/K/V per head, scaled dotâ€‘product, causal mask  
2. **Residual Connections** & **LayerNorm**  
3. **Feedâ€‘Forward Network** (two linear layers + ReLU)  
4. **Dropout** between subâ€‘layers  
5. **Final linear projection** to vocabulary size  

---

## ðŸ“ˆ Training

The model is trained to predict the next token in the Tiny Shakespeare corpus.

# Run training
python Train.py

Dataset: Tiny Shakespeare (via HuggingFace)
Loss: CrossEntropy
Optimizer: AdamW (lr=3eâ€‘4)
Epochs: 25
Batch Size: 16
Sequence Length: 64
Model Dim (d_model): 128
Heads: 4
After training, weights are saved to:
tiny_transformer_weights.pth


## ðŸŽ¨ Attention Visualization
Inspect how each attention head focuses across tokens:
-atten_image

![Transformer Decoder Architecture](https://raw.githubusercontent.com/Umang-projects/-Transformer-Decoder-from-Scratch-Tiny-Shakespeare-/main/atten_image.png)


## ðŸ¤” Why I Built This
-I didnâ€™t want to just use transformersâ€”I wanted to understand them endâ€‘toâ€‘end:
-Reinforce the theory behind attention mechanisms.
-Practice raw PyTorch modeling without highâ€‘level abstractions.
-Create reusable building blocks for GPTâ€‘style models.
